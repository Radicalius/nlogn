> August 16, 2020
# Web Scraping New Grad Software Engineering Jobs
[repo](repo://job-scraper)
---

One thing that I've learned during my job hunt is that many online job boards are not great at identifying roles for new grads.  Even LinkedIn, which has a filter option for experience level, still recommends me senior level jobs.  That being said, the jobs posted on LinkedIn and other popular online job-boards have many relevant postings; it's just sometimes hard to sort through the noise.  I would love to be able filter jobs from online job-boards with my own fine-grain search criteria.  So I decided to use web scraping to compile a list of jobs from some major online job boards and filter out jobs that I'm not qualified for.  The following describes my approach, as well as its successes and failures.

## My Web Scraping Toolkit

* [Firefox Developer Console](https://developer.mozilla.org/en-US/docs/Tools/Web_Console) - shows a lot of very helpful information about the DOM structure of a page, as well as the network requests made by the page.  This can be useful for figuring out how to query a page endpoint or identify a specific element on the page.  The easiest way to open it is by right-clicking on a page element of interest then clicking inspect element in the dropdown.  There are analogous tools on chrome which can accessed in much the same way.
* [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) - the iconic python web-scraping library.  It makes it easy to extract relevant information from html code.
* [Requests](https://requests.readthedocs.io/en/master/) - a python module for generating http requests.  This comes in handy for querying the website of interest.

## Scraping Online Job Boards

If you can display a page in your web browser, you should be able to scrape information from that page.  Unfortunately, the process is not always trivial.  Sometimes difficulty arises out of the complexity of the website.  In other cases, webmasters introduce complexity on purpose to deter scrapers.  The job boards I looked at fell into three categories in terms of difficulty to scrape:

1. [Indeed](https://www.indeed.com/) and [Monster](https://www.monster.com/) were very easy to scrape from.  They both required that the requests were made with believable `User-Agent` and `Referrer` headers, but other than that, there was no additional friction.
2. [LinkedIn](https://www.linkedin.com/) and [Dice](https://www.dice.com/) use `CSRF` and `API` tokens respectively.  However, by loading the search page to get the appropriate cookies and setting relevant headers, it was still possible to scape them using just the tools above.
3. [GlassDoor](https://www.glassdoor.com/index.htm) also uses `CSRF` tokens, but they are generated by a script instead of sent as cookies by the server.  The script is obfuscated, making it difficult to determine how exactly the token is generated.  Scraping Glassdoor would likely require something like [Selenium](https://selenium-python.readthedocs.io/).

The Firefox Developer Console is an invaluable tool for examining how to query a page of interest.  Look under the network tab, and you'll find a comprehensive list of headers, cookies, and payload for each request made by the browser.  For the most part, copying what the browser does exactly using the `requests` module will get you your html document.  Once you have the html source, it shouldn't be too hard to extract the relevant information using `BeautifulSoup`.  I won't bore you with the exact details of how to scrape from each of the online job-boards above.  If your interested in the specifics, please refer to my [github_repo](https://github.com/Radicalius/job_scraper).  Relevant code can be found in `/handlers/*Handler.py`

## Web Scraper Architecture

Making http requests can be rather slow.  The job boards I examined during this project consistently took roughly 1-2 seconds to respond.  If you are scarping jobs serially, this adds up quickly.  I found that scraping dice (~4000 jobs) took over 2 hours.  So I modified my scraper architecture to utilize a parallelized pipeline.  The design looks something like this.

![arch](/img/scraper.png)

1. The Search Result Parser scans each page of the results and extracts urls to job posting urls.  It then distributes them among the...
2. Posting Parsers, which request each job posting page and extract relevant information on the page about the job.  They then send it to the...
3. Filter, which determines whether new graduates might qualify for this job.  If it decides that the job is relevant, it passes it on to the...
4. Writer, which appends the job to a csv file.

Each stage of the pipeline has a queue from the previous stage that it reads to and a queue to the next stage that it writes to.  All of the workers operate asynchronously of each other.  The benefits of this design are that each stage of the pipeline can be scaled easily by adding / removing workers, and that the code has a nice modular structure.  The major disadvantage is that it requires a significant amount of overhead to keep track of all the threads.  I was able to scrape 240 jobs per minute with 1 Search Result Parser, 10 Posting Parsers, 1 Filter, and 1 Writer.  Since all of the thread's runtimes are IO-bound, I might be able to get better performance by scaling up some of the pipeline stages, but I have not experimented with that.

## Filtering the Results

The first thing I want to filter by is job title.  I read up on common software engineering job titles and found that the following experienced positions are rather prevalent in my scraped job lists.  

* Senior (aka Sr.) Software Engineer
* Staff Software Engineer
* Lead Software Engineer
* Chief Software Engineer
* Principal (sometimes misspelled as Principle) Software
* Software Engineer II, III, IV, etc.
* Mid-Level (aka Intermediate Level) Software Engineer
* Engineering Manager
* Vice President (aka VP) of Engineering
* etc       

All of the above were relatively easy to filter out using keywords.  I also used keywords to filter out positions that required a masters degree.  At this point the job list was looking pretty good overall.  There were still a bunch of jobs titled "Software Engineer" that asked for years of professional experience doing `X`, `Y`, and `Z`.  These requirements were not worded as consistently as the job titles, so keyword filtering didn't work well in this case.  I tried using the following regex as a filter:

```python
r"\d+.*years"
```

This partially worked, but it didn't catch cases where people spelled out the number (e.g. `five` vs `5`).  Amending the regex to the following did the trick:

```python
r"(\d+|one|two|three|four|five|six|seven|eight|nine|ten|.* years"
```

Applying all these filters together removed most of the irrelevant jobs, as well as some relevant ones.

## Conclusions and Extensions

This scraper does a reasonable job of collecting new grad software engineering jobs.  If I have more time to work on this project further, I'd look into the following additions:

* A User Interface - It would be really nice to have a clean frontend to view and search the jobs.  The scraper currently spits out a csv which is a pain to navigate.
* Using Selenium web browser to scrape more challenging sites like Glassdoor and AngelList.
* Experimenting with [aiohttp](https://docs.aiohttp.org/en/stable/) for potentially faster parallel web requests than the current pipeline
* Using Machine Learning to filter jobs.  (The biggest hurdle here is getting labeled data.  I may have to do this manually, in which case this might be too time intensive)     
